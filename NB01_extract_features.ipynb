{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXgJ6Q21CDA1",
        "outputId": "858427fe-a621-4030-aa80-01f5575d37c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-it27lpp2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-it27lpp2\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m824.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369500 sha256=28cf75d439df8cdd77014cb3b4d1b5875e8983eca08a66172be0cba167c11a6f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wlgq3slr/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from glob import glob\n",
        "from os import path\n",
        "import torch\n",
        "import clip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "VlMO7Eh2CUSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Drive"
      ],
      "metadata": {
        "id": "SIKDtfPGCa9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "3_fnGxF9CdaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is where my dataset lives"
      ],
      "metadata": {
        "id": "U_0QgENqCvU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_location = '/content/gdrive/MyDrive/nln-dataset'"
      ],
      "metadata": {
        "id": "7G30YHyOC2JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract CLIP features\n",
        "\n",
        "In this step CLIP features are extracted from the images in the dataset. The result is stored in clip-features.csv"
      ],
      "metadata": {
        "id": "UEWfBAE2C_w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_location_normalized = f\"{dataset_location}/normalized\"\n",
        "\n",
        "if not path.exists(dataset_location_normalized):\n",
        "    print(\"run NB00_normalize first\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "to_process = [sorted(glob(f'{label}/*')) for label in sorted(glob(f\"{dataset_location_normalized}/*\"))]\n",
        "to_process_flat = [item for lst in to_process for item in lst]\n",
        "\n",
        "labels = sorted(set([path.dirname(path.relpath(x, dataset_location_normalized)) for x in to_process_flat]))\n",
        "print(labels)\n",
        "\n",
        "images = [preprocess(Image.open(i)).unsqueeze(0).to(device) for i in tqdm(to_process_flat)]\n",
        "\n",
        "print(\"encoding images\")\n",
        "with torch.no_grad():\n",
        "    features = [model.encode_image(i).cpu().numpy().reshape(512) for i in images]\n",
        "\n",
        "fa = np.array(features)\n",
        "\n",
        "labeling = [labels.index(path.dirname(path.relpath(x, dataset_location_normalized))) for x in to_process_flat]\n",
        "\n",
        "identifier = [path.relpath(x, dataset_location_normalized) for x in to_process_flat]\n",
        "identifier_df = pd.DataFrame(identifier)\n",
        "identifier_df = identifier_df.rename(columns={0: \"id\"})\n",
        "\n",
        "label_df = pd.DataFrame(labeling)\n",
        "label_df = label_df.rename(columns={0: \"label\"})\n",
        "\n",
        "df = pd.DataFrame(fa)\n",
        "df = pd.concat([identifier_df, label_df, df], axis=1)\n",
        "\n",
        "df.to_csv(f\"{dataset_location}/clip-features.csv\", index=False)"
      ],
      "metadata": {
        "id": "HVWB7bwoC8I0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}